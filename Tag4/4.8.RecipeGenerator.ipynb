{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "interstate-dance",
   "metadata": {},
   "source": [
    "# Hier baue ich einen Rezeptgenerator basierend auf einem besseren Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-tackle",
   "metadata": {},
   "source": [
    "Der Tokenizer soll\n",
    "* Komplett umkehrbar sein: Tokenizer-Decoder\n",
    "* Damit kann ich die Korrektheit überprüfen\n",
    "* Und die generierten Rezepte haben die volle Ausdrucksfähigkeit\n",
    "* Groß- und Kleinschreibung wird über [LOWER][UPPER] Token kodiert\n",
    "* Stemming wird durchgeführt\n",
    "* Punctuation wird als special tokens separiert\n",
    "* Whitespace wird über [WHITESPACE] und [ANTISPACE] kodiert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-distribution",
   "metadata": {},
   "source": [
    "Die Rezepte\n",
    "* Enden mit [STOP]\n",
    "* Die Instructions starten mit [INSTRUCTIONS]\n",
    "* Komische Whitespaces werden gecleant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "changed-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#from tensorflow.keras.layers import TextVectorization\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daily-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuation(string):\n",
    "    \"\"\"\n",
    "    Setzt strategisch token und whitespace ein \n",
    "    damit nachher durch .split() auch die Satzzeichen zu Token werden.\n",
    "    \"\"\"\n",
    "    string = string.replace('\\r\\n\\r\\n','∆') # multi bytes ersetzen\n",
    "    string = string.replace('\\r\\n','Ω') # multi bytes ersetzen\n",
    "    string = string.replace('\\n','ª') # multi bytes ersetzen\n",
    "    string = string.replace('\\r','µ') # multi bytes ersetzen\n",
    "    \n",
    "    substrings = ['(', ')', '/', '&', '-', ',', '.', '!', '?', '\"', '°', '∆',';',':','Ω','ª','µ']\n",
    "    \n",
    "    new_string = ''\n",
    "    for i,b in enumerate(string):\n",
    "        if b in substrings:\n",
    "            if i>0 and string[i-1]==' ':\n",
    "                new_string += b\n",
    "            else: \n",
    "                new_string += ' [ANTISPACE] '\n",
    "                new_string += b\n",
    "        elif b == ' ':\n",
    "            if i==0 or string[i-1]==' ' or i==len(string)-1: # es ist ein zusätzlicher whitespace\n",
    "                new_string += '[WHITESPACE] '  # sonst wird der doppelte Whitespace durch split() ignoriert\n",
    "            else:\n",
    "                new_string += b\n",
    "        else: # b nicht ' ' und kein Punkt\n",
    "            if i>0 and string[i-1] in substrings: \n",
    "                new_string += ' [ANTISPACE] '\n",
    "                new_string += b\n",
    "            else:\n",
    "                new_string += b\n",
    "    return new_string\n",
    "    \n",
    "\n",
    "def de_punctuation(string):\n",
    "    \"\"\"\n",
    "    Macht punctuation komplett wieder rückgängig.\n",
    "    \"\"\"\n",
    "    string = string.replace(' [ANTISPACE] ','')\n",
    "    string = string.replace('[WHITESPACE]','')  # [WHITESPACE] liefert den whitespace wie jedes Token mit\n",
    "    \n",
    "    string = string.replace('∆','\\r\\n\\r\\n') # multi bytes ersetzen\n",
    "    string = string.replace('Ω','\\r\\n') # multi bytes ersetzen\n",
    "    string = string.replace('ª','\\n') # multi bytes ersetzen\n",
    "    string = string.replace('µ','\\r') # multi bytes ersetzen\n",
    "    \n",
    "    return string\n",
    "\n",
    "\n",
    "def split(string):\n",
    "    return string.split()\n",
    "\n",
    "def de_split(liste):\n",
    "    string = ' '.join(liste)\n",
    "    if string.endswith('[WHITESPACE]'):\n",
    "        string += ' '\n",
    "    return string\n",
    "\n",
    "def capitalize(liste, most_common_capitalization):\n",
    "    new_liste = []\n",
    "    for word in liste:\n",
    "        upper = word[0].upper() + word[1:]\n",
    "        lower = word[0].lower() + word[1:]\n",
    "        \n",
    "        if upper in most_common_capitalization:\n",
    "            if word == upper:\n",
    "                new_liste.append(upper)\n",
    "            elif word == lower:\n",
    "                new_liste.append('[LOWER]')\n",
    "                new_liste.append(upper)\n",
    "        else: # lower ist die Normalform\n",
    "            if word == upper:\n",
    "                new_liste.append('[UPPER]')\n",
    "                new_liste.append(lower)\n",
    "            elif word == lower:\n",
    "                new_liste.append(lower)\n",
    "    return new_liste\n",
    "\n",
    "def de_capitalize(liste):\n",
    "    new_liste = []\n",
    "    up  = False  # vorheriges wort war lower token\n",
    "    low = False  # vorheriges word war upper token\n",
    "    \n",
    "    for word in liste:\n",
    "        if word == '[LOWER]':\n",
    "            low = True\n",
    "            up  = False\n",
    "        elif word == '[UPPER]':\n",
    "            up  = True\n",
    "            low = False\n",
    "        \n",
    "        else:\n",
    "            if up == True:\n",
    "                new_liste.append(word[0].upper()+word[1:])\n",
    "            elif low == True:\n",
    "                new_liste.append(word[0].lower()+word[1:])\n",
    "            else:\n",
    "                new_liste.append(word)\n",
    "            low = False\n",
    "            up  = False\n",
    "    return new_liste\n",
    "                \n",
    "def stem(liste, stemming_dict):\n",
    "    new_liste = []\n",
    "    suffixe = ['en','st','es','em','er','e','t']\n",
    "    for word in liste:\n",
    "        no_suffix = True\n",
    "        for suffix in suffixe:\n",
    "            if word.endswith(suffix) and word[:-1*len(suffix)] in stemming_dict:\n",
    "                new_liste.append(word[:-1*len(suffix)])\n",
    "                new_liste.append('§$'+suffix)\n",
    "                no_suffix = False\n",
    "                break\n",
    "        if no_suffix:\n",
    "            new_liste.append(word)\n",
    "    return new_liste\n",
    "\n",
    "def de_stem(liste):\n",
    "    new_liste = []\n",
    "    for token in liste:\n",
    "        if len(token)>1 and token[:2]=='§$' and new_liste:\n",
    "            new_liste[-1] += token[2:]\n",
    "        else:\n",
    "            new_liste.append(token)\n",
    "    return new_liste\n",
    "\n",
    "def numberize(liste, token_to_number):\n",
    "    return [token_to_number[token] if token in token_to_number else token_to_number['[UKN]'] for token in liste]\n",
    "\n",
    "def de_numberize(liste, number_to_token):\n",
    "    return [number_to_token[number] for number in liste]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "superior-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "        Turns recipes into sequences of numbers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, token_no=10000, validation_size=1000):\n",
    "        \n",
    "        # Rezepte erstellen:\n",
    "        self.recipes       = []\n",
    "        self.extract_recipes(df)\n",
    "        random.Random(0).shuffle(self.recipes)\n",
    "        \n",
    "        self.validation_size = validation_size\n",
    "        \n",
    "        self.token_number      = token_no\n",
    "        \n",
    "        self.suffixe       = ['en','st','es','em','er','e','t']\n",
    "        self.suffix_count  = {'en':0, 'st':0, 'es':0, 'em':0, 'er':0, 'e':0, 't':0}\n",
    "        self.symbols       = ['.', '!', '?', '∆',':','Ω','ª','µ'] \n",
    "        \n",
    "        \n",
    "        self.word_count = {}\n",
    "        self.stemmed_count    = {}\n",
    "        self.counting()\n",
    "        \n",
    "        self.most_common_capitalization = {}\n",
    "        self.most_common_case()\n",
    "        \n",
    "        self.stemming_dict = {}\n",
    "        self.token_to_number = {}\n",
    "        self.number_to_token = []\n",
    "        self.compute_stems()\n",
    "        self.compute_tokens()\n",
    "        \n",
    "        # saving precomputed data for tokenization: \n",
    "        self.precomputed_dicts = (self.stemming_dict,\n",
    "                                  self.most_common_capitalization,\n",
    "                                  self.token_to_number,\n",
    "                                  self.number_to_token)\n",
    "        json_string = json.dumps(self.precomputed_dicts)\n",
    "        with open('precomputed_dicts.json','w') as f:\n",
    "            f.write(json_string)\n",
    "        \n",
    "        \n",
    "    def extract_recipes(self, df):\n",
    "        \"\"\"\n",
    "            Extracts recipes from the dataframe \n",
    "        \"\"\"\n",
    "        for r in range(len(df)):\n",
    "            try: \n",
    "                ingredients  = df.recipeIngredient.iloc[r][2:-2].split(\"', '\")\n",
    "                instructions = df.recipeInstructions.iloc[r]\n",
    "                recipe = '\\r\\n\\r\\n'.join(ingredients) + '\\r\\n\\r\\n' + ' [INSTRUCTIONS] ' + instructions\n",
    "                \n",
    "                recipe = recipe.replace('\\u2028',' ') # Traurige Notwendigkeit\n",
    "                recipe = recipe.replace('\\u2009',' ') # Traurige Notwendigkeit\n",
    "                recipe = recipe.replace('\\u2003',' ') # Traurige Notwendigkeit\n",
    "                recipe = recipe.replace('\\u2007',' ') # Traurige Notwendigkeit\n",
    "                recipe = recipe.replace('\\u2005',' ') # Traurige Notwendigkeit\n",
    "                recipe = recipe.replace('\\u202f',' ') # Traurige Notwendigkeit\n",
    "                recipe = recipe.replace('\\u2029',' ') # Traurige Notwendigkeit\n",
    "                recipe = recipe.replace('\\t',' ')     # Traurige Notwendigkeit\n",
    "                recipe = recipe.replace('\\xa0',' ')   # Traurige Notwendigkeit\n",
    "                recipe = recipe.replace('\\x1e',' ')   # Traurige Notwendigkeit\n",
    "                recipe = recipe.replace('\\x0b',' ')   # Traurige Notwendigkeit\n",
    "                \n",
    "                self.recipes.append(recipe)\n",
    "            except:\n",
    "                print(\"Skipping recipe\",r)\n",
    "                print(df.loc[r].recipeIngredient)\n",
    "                print(df.loc[r].recipeInstructions)\n",
    "                print()\n",
    "        print(\"{} recipes extracted.\".format(len(self.recipes)))\n",
    "        \n",
    "        \n",
    "    def counting(self):\n",
    "        \"\"\"\n",
    "        Hier werden direkt naive_words, stemmed_parts, capitalization gezählt\n",
    "        Und dann das stemming_dict + token_dict etc berechnet.\n",
    "        \"\"\"\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # wörter und stemmed wörter werden gezählt\n",
    "        for recipe in tqdm(self.recipes):\n",
    "            punc = punctuation(recipe)\n",
    "            naive_words = punc.split()\n",
    "            for i,word in enumerate(naive_words):\n",
    "                if word in self.word_count:\n",
    "                    self.word_count[word] += 1\n",
    "                else:\n",
    "                    self.word_count[word]  = 1\n",
    "\n",
    "                for suffix in self.suffixe:\n",
    "                    if len(word)-len(suffix)>2:\n",
    "                        if word.endswith(suffix):\n",
    "                            self.suffix_count[suffix] += 1\n",
    "                            if word[:-1*len(suffix)] in self.stemmed_count:\n",
    "                                self.stemmed_count[word[:-1*len(suffix)]] += 1\n",
    "                            else:\n",
    "                                self.stemmed_count[word[:-1*len(suffix)]]  = 1\n",
    "        print(\"{} minutes for counting words.\".format((time.time()-start_time)/60))   \n",
    "        \n",
    "    def most_common_case(self):\n",
    "        # Ist Groß- oder Kleinschreibung der Normalfall für ein Wort?\n",
    "        for word in self.word_count:\n",
    "            upper = word[0].upper() + word[1:]\n",
    "            lower = word[0].lower() + word[1:]\n",
    "            upcount  = 0\n",
    "            lowcount = 0\n",
    "            if upper in self.word_count:\n",
    "                upcount = self.word_count[upper]\n",
    "            if lower in self.word_count:\n",
    "                lowcount = self.word_count[lower]\n",
    "            if lower == upper: \n",
    "                lowcount = 0 # sonst wird doppelt gezählt\n",
    "\n",
    "            if upcount > lowcount:\n",
    "                self.most_common_capitalization[upper] = upcount + lowcount\n",
    "            else:\n",
    "                self.most_common_capitalization[lower] = upcount + lowcount\n",
    "        \n",
    "        print('Most common capitalization computed.')\n",
    "                \n",
    "                \n",
    "    def compute_stems(self):\n",
    "        \"\"\"\n",
    "        Wir vergleichen den count von stems mit dem count des ganzen Wortes\n",
    "        Falls der stem doppelt so häufig ist, sagen wir das stemming eine gute Idee ist.\n",
    "        D.h. das der stem in das stemming_dict kommt.\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        for word in self.most_common_capitalization:  # wir brauchen nur eine schreibweise\n",
    "            for suffix in self.suffixe:\n",
    "                if word.endswith(suffix) and len(word)-len(suffix)>2:\n",
    "                    if self.stemmed_count[word[:-1*len(suffix)]] > 2*self.most_common_capitalization[word]:\n",
    "                        self.stemming_dict[word[:-1*len(suffix)]] = True\n",
    "                        break  # Nur eine Form des Stemming 'st' statt 't' z.B.\n",
    "        # Jetzt kennen wir das stemming und zählen wörter neu:\n",
    "        self.word_count = {}\n",
    "        for word in self.most_common_capitalization:  # wir brauchen nur eine schreibweise\n",
    "            potentially_stemmed_word = word\n",
    "            for suffix in self.suffixe:\n",
    "                if word.endswith(suffix) and word[:-1*len(suffix)] in self.stemming_dict:\n",
    "                    potentially_stemmed_word = word[:-1*len(suffix)]\n",
    "                    break  # Nur eine Form des Stemming 'st' statt 't' z.B.\n",
    "            if potentially_stemmed_word in self.word_count: # stems kommen öfter vor\n",
    "                self.word_count[potentially_stemmed_word] += self.most_common_capitalization[word]\n",
    "            else:\n",
    "                self.word_count[potentially_stemmed_word]  = self.most_common_capitalization[word]\n",
    "        # Dieses word_count enthält nun die tokens nach capitalization und stemming\n",
    "        print(len(self.stemmed_count),'stems chosen.')\n",
    "        \n",
    "    def compute_tokens(self):\n",
    "        \"\"\"\n",
    "        Hier zerlegen wir die gezählten Wörter mit stemming und in den most_common_capitalization case\n",
    "        Dann sortieren wir nach count und nehmen die häufigsten dieser Teile ins token_dict auf.\n",
    "        \"\"\"\n",
    "\n",
    "        self.word_count = dict(sorted(self.word_count.items(), key=lambda item: item[1])[::-1][:self.token_number-12])\n",
    "        self.token_to_number = {'[UKN]':0,'[STOP]':1,'[UPPER]':2,'[LOWER]':3,'[INSTRUCTIONS]':4}\n",
    "        count = 5\n",
    "\n",
    "        self.number_to_token = ['[UKN]','[STOP]','[UPPER]','[LOWER]','[INSTRUCTIONS]']\n",
    "\n",
    "        # suffixe für das stemming zuerst.\n",
    "        for token in ['§$en','§$st','§$es','§$em','§$er','§$e','§$t']: \n",
    "            self.token_to_number[token] = count\n",
    "            self.number_to_token.append(token)\n",
    "            count += 1\n",
    "\n",
    "        for token in self.word_count:\n",
    "            if token not in self.token_to_number:\n",
    "                self.token_to_number[token] = count\n",
    "                self.number_to_token.append(token)\n",
    "                count += 1\n",
    "\n",
    "        print(len(self.token_to_number),'tokens chosen.')\n",
    "        \n",
    "        \n",
    "    def tokenize(self,string):\n",
    "        punc = punctuation(string)\n",
    "        liste = split(punc)\n",
    "        capped_liste = capitalize(liste,self.most_common_capitalization)\n",
    "        stemmed = stem(capped_liste,self.stemming_dict)\n",
    "        tokens = numberize(stemmed,self.token_to_number)\n",
    "        return tokens\n",
    "\n",
    "    def de_tokenize(self,tokens):\n",
    "        liste  = de_numberize(tokens,self.number_to_token)\n",
    "        liste  = de_stem(liste)\n",
    "        liste  = de_capitalize(liste)\n",
    "        string = de_split(liste)\n",
    "        string = de_punctuation(string)\n",
    "        return string\n",
    "\n",
    "\n",
    "    def generator(self, batchsize, seqlen):\n",
    "        batch_list = []\n",
    "        while True:\n",
    "            for recipe in self.recipes[self.validation_size:]:\n",
    "                tokens = self.tokenize(recipe)\n",
    "                tokens = tokens + [self.token_to_number['[STOP]']] * seqlen\n",
    "                tokens = np.array(tokens)\n",
    "                tokens = tokens.reshape(-1,len(tokens))\n",
    "                x = tokens[:,:seqlen]\n",
    "                y = tokens[:,1:seqlen+1]\n",
    "                batch_list.append((x,y))\n",
    "                if len(batch_list) == batchsize:\n",
    "                    xx = np.concatenate([x for (x,y) in batch_list])\n",
    "                    yy = np.concatenate([y for (x,y) in batch_list])\n",
    "                    batch_list = []\n",
    "                    yield (xx,yy)\n",
    "\n",
    "    def validation_generator(self, batchsize, seqlen):\n",
    "        batch_list = []\n",
    "        for recipe in self.recipes[:self.validation_size]:\n",
    "            tokens = self.tokenize(recipe)\n",
    "            tokens = tokens + [self.token_to_number['[STOP]']] * seqlen\n",
    "            tokens = np.array(tokens)\n",
    "            tokens = tokens.reshape(-1,len(tokens))\n",
    "            x = tokens[:,:seqlen]\n",
    "            y = tokens[:,1:seqlen+1]\n",
    "            batch_list.append((x,y))\n",
    "            if len(batch_list) == batchsize:\n",
    "                xx = np.concatenate([x for (x,y) in batch_list])\n",
    "                yy = np.concatenate([y for (x,y) in batch_list])\n",
    "                batch_list = []\n",
    "                yield (xx,yy)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "neither-disabled",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 recipes extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 10000/10000 [00:11<00:00, 846.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19690123399098713 minutes for counting words.\n",
      "Most common capitalization computed.\n",
      "15581 stems chosen.\n",
      "9999 tokens chosen.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('10000_Chefkoch_Rezepte.csv') \n",
    "tokey = Tokenizer(df)\n",
    "df = '' # Memory freigeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "historical-forth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 512) (32, 512)\n"
     ]
    }
   ],
   "source": [
    "for batch in tokey.generator(32,512):\n",
    "    print(batch[0].shape,batch[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "continuing-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ANTISPACE] 604578\n",
      ". 131552\n",
      "∆ 116151\n",
      ", 97698\n",
      "und 80510\n",
      "die 55876\n",
      "mit 35504\n",
      "ª 35384\n",
      "in 34738\n",
      "( 34594\n",
      ") 34588\n",
      "1 31239\n",
      "den 27962\n",
      "g 27877\n",
      "ein 27514\n",
      "n 23172\n",
      "2 17852\n",
      "[WHITESPACE] 17697\n",
      "der 17046\n",
      "- 16248\n",
      "EL 15345\n",
      "das 14534\n",
      "Salz 12829\n",
      "auf 11985\n",
      "lass 11539\n",
      "dem 11077\n",
      "[INSTRUCTIONS] 10000\n",
      "Pfeffer 9984\n",
      "oder 9430\n",
      "geb 9246\n",
      "schneid 9038\n",
      "Minut 8675\n",
      "ca 8576\n",
      "etwas 8537\n",
      "3 7774\n",
      "Zuck 7520\n",
      "TL 7124\n",
      "ml 6989\n",
      "klein 6643\n",
      "/ 6454\n",
      "nach 6284\n",
      "4 6256\n",
      "Wasser 6026\n",
      "dann 6015\n",
      "im 5985\n",
      "Butt 5872\n",
      "für 5821\n",
      "200 5478\n",
      "bei 5412\n",
      "Ω 5377\n",
      "Öl 4960\n",
      "zu 4915\n",
      "groß 4874\n",
      "bis 4797\n",
      "Scheib 4738\n",
      "zum 4671\n",
      "100 4667\n",
      "Zwiebel 4622\n",
      "Mehl 4551\n",
      "fein 4511\n",
      "aus 4500\n",
      "Sahne 4463\n",
      "gut 4192\n",
      "5 4157\n",
      "Pfann 4068\n",
      "ist 4011\n",
      "° 3955\n",
      "½ 3911\n",
      "Olivenöl 3826\n",
      "Tomat 3775\n",
      "schäl 3710\n",
      "B 3656\n",
      "Teig 3623\n",
      "Knoblauch 3621\n",
      "C 3576\n",
      "frisch 3536\n",
      "Min 3509\n",
      "all 3431\n",
      "wasch 3389\n",
      "Ei 3369\n",
      "250 3272\n",
      "verteil 3239\n",
      "back 3187\n",
      "koch 3124\n",
      "Milch 3112\n",
      "noch 3094\n",
      "abschmeck 3091\n",
      "er 3080\n",
      "von 3045\n",
      "Stück 3037\n",
      "servier 3036\n",
      "Backofen 3004\n",
      "unt 2998\n",
      "10 2933\n",
      "150 2882\n",
      "dazugeb 2844\n",
      "rühr 2829\n",
      "50 2815\n",
      "auch 2748\n",
      "nicht 2744\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for key in tokey.word_count:\n",
    "    print(key,tokey.word_count[key])\n",
    "    count += 1\n",
    "    if count == 100:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "metric-thesis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1254"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test was in word_count drin ist: [INSTRUCTIONS] etc\n",
    "tokey.word_count['Reis']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "missing-secondary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 g Kartoffel(n)\r\n",
      "\r\n",
      " Salz\r\n",
      "\r\n",
      " Bohnenkraut\r\n",
      "\r\n",
      "65 g Brechbohnen , TK\r\n",
      "\r\n",
      "1 kleine Birne(n)\r\n",
      "\r\n",
      "15 g Katenschinken , gewürfelt\r\n",
      "\r\n",
      "30 g Schmand\r\n",
      "\r\n",
      "15 g Milch\r\n",
      "\r\n",
      "1  Ei(er)\r\n",
      "\r\n",
      " Pfeffer , schwarz, aus der Mühle\r\n",
      "\r\n",
      "20 g Emmentaler , grob gerieben\r\n",
      "\r\n",
      "1 TL Rapsöl zum Einfetten der Form\r\n",
      "\r\n",
      " [INSTRUCTIONS] Die Kartoffeln schälen und klein würfeln. Die Würfel in einem Topf mit Salzwasser und etwas Bohnenkraut einmal aufkochen, dann die Bohnen dazugeben und alles abgedeckt 10 Minuten bei kleinerer Hitze köcheln lassen. Nach dieser Zeit das Gemüse über einem Sieb abgießen, mit kaltem Wasser abschrecken und zum Abtropfen über eine Schüssel hängen.\n",
      "\n",
      "Den Ofen auf 180 Grad vorheizen.\n",
      "\n",
      "In einer Schüssel Schmand, Milch und das Ei verrühren, mit Salz und Pfeffer abschmecken und etwas Bohnenkraut untermischen. Eine kleine Auflaufform einfetten.\n",
      "\n",
      "Kartoffeln und Bohnen unter die Schmandmischung heben und den Katenschinken untermischen. Die Birne schälen, entkernen, klein würfeln und untermengen. Die Masse in die Auflaufform umfüllen und mit dem geriebenen Käse bestreuen.\n",
      "\n",
      "Im Ofen ca. 20 Minuten überbacken, die Backzeit kann variieren.\n",
      "\n",
      "Den Auflauf auf einem Teller anrichten, garnieren und servieren.\n",
      "120 g Kartoffel(n)\r\n",
      "\r\n",
      " Salz\r\n",
      "\r\n",
      " Bohnenkraut\r\n",
      "\r\n",
      "65 g Brechbohnen , TK\r\n",
      "\r\n",
      "1 kleine Birne(n)\r\n",
      "\r\n",
      "15 g Katenschinken , gewürfelt\r\n",
      "\r\n",
      "30 g Schmand\r\n",
      "\r\n",
      "15 g Milch\r\n",
      "\r\n",
      "1  Ei(er)\r\n",
      "\r\n",
      " Pfeffer , schwarz, aus der Mühle\r\n",
      "\r\n",
      "20 g Emmentaler , grob gerieben\r\n",
      "\r\n",
      "1 TL Rapsöl zum Einfetten der Form\r\n",
      "\r\n",
      " [INSTRUCTIONS] Die Kartoffeln schälen und klein würfeln. Die Würfel in einem Topf mit Salzwasser und etwas Bohnenkraut einmal aufkochen, dann die Bohnen dazugeben und alles abgedeckt 10 Minuten bei kleinerer Hitze köcheln lassen. Nach dieser Zeit das Gemüse über einem Sieb abgießen, mit kaltem Wasser abschrecken und zum Abtropfen über eine Schüssel hängen.\n",
      "\n",
      "Den Ofen auf 180 Grad vorheizen.\n",
      "\n",
      "In einer Schüssel Schmand, Milch und das Ei verrühren, mit Salz und Pfeffer abschmecken und etwas Bohnenkraut untermischen. Eine kleine Auflaufform einfetten.\n",
      "\n",
      "Kartoffeln und Bohnen unter die Schmandmischung heben und den Katenschinken untermischen. Die Birne schälen, entkernen, klein würfeln und untermengen. Die Masse in die Auflaufform umfüllen und mit dem geriebenen Käse bestreuen.\n",
      "\n",
      "Im Ofen ca. 20 Minuten überbacken, die Backzeit kann variieren.\n",
      "\n",
      "Den Auflauf auf einem Teller anrichten, garnieren und servieren.\n"
     ]
    }
   ],
   "source": [
    "for recipe in tokey.recipes:\n",
    "    tokens = tokey.tokenize(recipe)\n",
    "    decode = tokey.de_tokenize(tokens)\n",
    "    print(recipe)\n",
    "    print(decode)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-cooperation",
   "metadata": {},
   "source": [
    "Das counting ist kompliziert, weil es sich nach jeden zusätzlich berechneten preprocessing schritt verändert. D.h. nach dem Stemming sind die Stems neu und die ganzen Wörter obsolet. Nach dem most_common_capitalization Schritt gibt es nur noch eine Schreibweise, aber der count der anderen Schreibweise ist noch relevant. \n",
    "\n",
    "Wie muss man das aufziehen? \n",
    "* capitalize kommt vor dem stemming d.h. stemming muss auf der einen Schreibweise basieren. \n",
    "* D.h. die naive_word_count wird einfach word_count und hier umgeformt zu einer Schreibweise mit Kombicount.\n",
    "* Dann wird stemming_dict berechnet.\n",
    "* stems müssen zum word_count hinzugefügt werden, gestemmte Wörter entfernt.\n",
    "* das sollte durch Umformen der dicts passieren, nicht durch neu-tokenisieren der Rezepte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "parliamentary-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transformer Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "possible-polls",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Mask the upper half of the dot product matrix in self attention.\n",
    "    This prevents flow of information from future tokens to current token.\n",
    "    1's in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = tf.cast(m, dtype)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "    )\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        \n",
    "        # rezero addition\n",
    "        self.alpha = tf.Variable( initial_value=1e-4, dtype=\"float32\",  trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.layernorm1(inputs + self.alpha * attention_output) # rezero addition\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + self.alpha * ffn_output) # rezero addition\n",
    "    \n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "silver-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000  # Only consider the top 10k words\n",
    "maxlen = 512  # Max sequence size\n",
    "embed_dim = 256  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "\n",
    "def create_model(no_of_layers):\n",
    "    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    \n",
    "    for t in range(no_of_layers):\n",
    "        transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
    "        x = transformer_block(x)\n",
    "    \n",
    "    outputs = layers.Dense(vocab_size)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(\n",
    "        \"adam\", loss=[loss_fn, None],\n",
    "    )  # No loss and optimization based on word embeddings from transformer block\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "vulnerable-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate text from a trained model.\n",
    "    1. Feed some starting prompt to the model\n",
    "    2. Predict probabilities for the next token\n",
    "    3. Sample the next token and add it to the next input\n",
    "\n",
    "    Arguments:\n",
    "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
    "        start_tokens: List of integers, the token indices for the starting prompt.\n",
    "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
    "        top_k: Integer, sample from the `top_k` token predictions.\n",
    "        print_every: Integer, print after this many epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n",
    "    ):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.start_tokens = start_tokens\n",
    "        self.index_to_word = index_to_word\n",
    "        self.print_every = print_every\n",
    "        self.k = top_k\n",
    "\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def detokenize(self, number):\n",
    "        return self.index_to_word[number]\n",
    "    \n",
    "    def de_tokenize(self,tokens):\n",
    "        liste  = de_numberize(tokens,self.index_to_word)\n",
    "        liste  = de_stem(liste)\n",
    "        liste  = de_capitalize(liste)\n",
    "        string = de_split(liste)\n",
    "        string = de_punctuation(string)\n",
    "        return string\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        start_tokens = [_ for _ in self.start_tokens]\n",
    "        if (epoch + 1) % self.print_every != 0:\n",
    "            return\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            pad_len = maxlen - len(start_tokens)\n",
    "            sample_index = len(start_tokens) - 1\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[:maxlen]\n",
    "                sample_index = maxlen - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "            x = np.array([x])\n",
    "            y, _ = self.model.predict(x)\n",
    "            sample_token = self.sample_from(y[0][sample_index])\n",
    "            tokens_generated.append(sample_token)\n",
    "            start_tokens.append(sample_token)\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "            \n",
    "        txt = \" \".join(\n",
    "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
    "        )\n",
    "        txt = txt.split('[STOP]')[0]\n",
    "        line_liste = txt.split('\\r\\n')\n",
    "        print(f\"generated text:\\n\")\n",
    "        for line in line_liste:\n",
    "            print(line)\n",
    "        print()\n",
    "        \n",
    "        print(\"The other detokenizer ...\")\n",
    "        txt = self.de_tokenize(self.start_tokens + tokens_generated)\n",
    "        txt = txt.split('[STOP]')[0]\n",
    "        line_liste = txt.split('\\r\\n')\n",
    "        for line in line_liste:\n",
    "            print(line)\n",
    "        print()\n",
    "        \n",
    "# Tokenize starting prompt\n",
    "\n",
    "start_prompt = \"250 g Reis\"\n",
    "start_tokens = [tokey.token_to_number.get(_, 1) for _ in start_prompt.split()]\n",
    "num_tokens_generated = 250\n",
    "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, tokey.number_to_token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-aurora",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "decent-development",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 512)]             0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, 512, 256)         2691072   \n",
      " g_1 (TokenAndPositionEmbedd                                     \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " transformer_block_16 (Trans  (None, 512, 256)         658689    \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " transformer_block_17 (Trans  (None, 512, 256)         658689    \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " transformer_block_18 (Trans  (None, 512, 256)         658689    \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " transformer_block_19 (Trans  (None, 512, 256)         658689    \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 512, 10000)        2570000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,895,828\n",
      "Trainable params: 7,895,828\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(4)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-input",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 2.9621 - dense_41_loss: 2.9621generated text:\n",
      "\n",
      "250 g Reis , ( [ANTISPACE] s [ANTISPACE] ) [ANTISPACE] ∆ [ANTISPACE] ∆ [ANTISPACE] ∆ [ANTISPACE] ) [ANTISPACE] 1 Dos §$e [ANTISPACE] n [ANTISPACE] ∆ [ANTISPACE] ) [ANTISPACE] ∆ [ANTISPACE] , ( [ANTISPACE] n [ANTISPACE] n [ANTISPACE] ∆ [ANTISPACE] ) [ANTISPACE] ∆ [ANTISPACE] / [ANTISPACE] ∆ [ANTISPACE] , gehack §$t [ANTISPACE] ∆ [ANTISPACE] 1 EL Sojasauc §$e [ANTISPACE] ∆ [ANTISPACE] ) , fein gehack §$t [ANTISPACE] ( [ANTISPACE] . [ANTISPACE] 2 cl [ANTISPACE] ∆ [ANTISPACE] ( [ANTISPACE] n [ANTISPACE] n [ANTISPACE] , rot [ANTISPACE] ∆ [ANTISPACE] n [ANTISPACE] ∆ Salz und mit dem Öl [ANTISPACE] . [ANTISPACE] ) [ANTISPACE] 1 [WHITESPACE] Zitron §$e in die Sauc §$e , den Ofen nehm §$en [ANTISPACE] - [ANTISPACE] . B [ANTISPACE] . [UPPER] die Banan §$en und die Zwiebeln [ANTISPACE] . [UPPER] dann die Banan §$en lass §$en und all §$es [ANTISPACE] ª [ANTISPACE] ∆ [ANTISPACE] , bis es nicht mehr [ANTISPACE] ª [ANTISPACE] , frisch §$em Wasser in ein §$e mit ein §$e schneid §$en [ANTISPACE] 1 groß §$e mit ein §$em Sieb abtropf §$en lass §$en schneid §$en [ANTISPACE] ∆ [ANTISPACE] . [UPPER] dann mit dem Zuck §$er [ANTISPACE] . [UPPER] mit ein §$er in Streif §$en [ANTISPACE] . [ANTISPACE] . [UPPER] im Ofen auf der Pfann §$e geb §$en [ANTISPACE] ª [ANTISPACE] , die Nudeln nach Belieben mit den Reis in Scheib §$en [ANTISPACE] . [UPPER] die Hälft §$e [ANTISPACE] [UPPER] die Pfann §$e mit Zuck §$er und in den [UKN] und die Zwiebeln in Scheib §$en [ANTISPACE] . [UPPER] den restlich §$e und\n",
      "\n",
      "The other detokenizer ...\n",
      "250 g Reis , (s)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ")1 Dosen\n",
      "\n",
      ")\n",
      "\n",
      ", (nn\n",
      "\n",
      ")\n",
      "\n",
      "/\n",
      "\n",
      ", gehackt\n",
      "\n",
      "1 EL Sojasauce\n",
      "\n",
      ") , fein gehackt(.2 cl\n",
      "\n",
      "(nn, rot\n",
      "\n",
      "n\n",
      "\n",
      " Salz und mit dem Öl.)1  Zitrone in die Sauce , den Ofen nehmen-. B. Die Bananen und die Zwiebeln. Dann die Bananen lassen und alles\n",
      "\n",
      "\n",
      ", bis es nicht mehr\n",
      ", frischem Wasser in eine mit eine schneiden1 große mit einem Sieb abtropfen lassen schneiden\n",
      "\n",
      ". Dann mit dem Zucker. Mit einer in Streifen.. Im Ofen auf der Pfanne geben\n",
      ", die Nudeln nach Belieben mit den Reis in Scheiben. Die HälfteDie Pfanne mit Zucker und in den [UKN] und die Zwiebeln in Scheiben. Den restliche und\n",
      "\n",
      "100/100 [==============================] - 457s 5s/step - loss: 2.9621 - dense_41_loss: 2.9621\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 1.7303 - dense_41_loss: 1.7303generated text:\n",
      "\n",
      "250 g Reis ( [ANTISPACE] ( [ANTISPACE] 1 m [ANTISPACE] ∆ [ANTISPACE] , getrocknet [ANTISPACE] ∆ [ANTISPACE] ( [ANTISPACE] ) [ANTISPACE] ∆ [ANTISPACE] ½ [WHITESPACE] Knoblauchzeh §$e [ANTISPACE] n [ANTISPACE] ) [ANTISPACE] ∆ [ANTISPACE] ( [ANTISPACE] n [ANTISPACE] ∆ [ANTISPACE] ∆ [ANTISPACE] n [ANTISPACE] 1 TL [ANTISPACE] ∆ [ANTISPACE] ∆ [ANTISPACE] 1 [WHITESPACE] Ei [ANTISPACE] ∆ [ANTISPACE] ∆ [ANTISPACE] ( [ANTISPACE] ∆ [ANTISPACE] 1 groß §$e [ANTISPACE] 2 [WHITESPACE] Zitron §$e [ANTISPACE] , Pfeffer [ANTISPACE] 2 TL [ANTISPACE] ∆ [ANTISPACE] , Pfeffer , gerieben [ANTISPACE] ( [ANTISPACE] ∆ [ANTISPACE] 100 g [ANTISPACE] ∆ [ANTISPACE] ∆ [ANTISPACE] ( [ANTISPACE] ∆ [ANTISPACE] ∆ [ANTISPACE] ∆ [ANTISPACE] ∆ [ANTISPACE] ∆ [ANTISPACE] 2 EL Sojasauc §$e [ANTISPACE] ∆ [ANTISPACE] ∆ [ANTISPACE] . B [ANTISPACE] 1 Bund Basilikum , Salz und Pfeffer würz §$en und in der Dos §$e schneid §$en [ANTISPACE] . [UPPER] die [UKN] und in ein §$en und Pfeffer abschmeck §$en [ANTISPACE] , in den Knoblauch fein §$e in der Form auf dem Glas geb §$en [ANTISPACE] : [UPPER] den Saft der Mühle würz §$en [ANTISPACE] , den Gewürz §$e Auflaufform leg §$en [ANTISPACE] . [UPPER] mit ein §$em [UKN] in den Teig auf 180 ° [ANTISPACE] [UPPER] die Butt §$er [ANTISPACE] . [ANTISPACE] ª [ANTISPACE] . [ANTISPACE] . [UPPER] das Wasser und Pfeffer , bis die Sauc §$e geb §$en und in ein §$er in ein §$em Schneebesen misch §$en [ANTISPACE] . [UPPER] die Zwiebel [ANTISPACE] . [UPPER] das Gemüs §$e in der Zwischenzeit die Butt §$er Pfann §$e [ANTISPACE] . [UPPER] das Gemüs §$e\n",
      "\n",
      "The other detokenizer ...\n",
      "250 g Reis ((1 m\n",
      "\n",
      ", getrocknet\n",
      "\n",
      "()\n",
      "\n",
      "½  Knoblauchzehen)\n",
      "\n",
      "(n\n",
      "\n",
      "\n",
      "\n",
      "n1 TL\n",
      "\n",
      "\n",
      "\n",
      "1  Ei\n",
      "\n",
      "\n",
      "\n",
      "(\n",
      "\n",
      "1 große2  Zitrone, Pfeffer2 TL\n",
      "\n",
      ", Pfeffer , gerieben(\n",
      "\n",
      "100 g\n",
      "\n",
      "\n",
      "\n",
      "(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2 EL Sojasauce\n",
      "\n",
      "\n",
      "\n",
      ". B1 Bund Basilikum , Salz und Pfeffer würzen und in der Dose schneiden. Die [UKN] und in einen und Pfeffer abschmecken, in den Knoblauch feine in der Form auf dem Glas geben: Den Saft der Mühle würzen, den Gewürze Auflaufform legen. Mit einem [UKN] in den Teig auf 180 °Die Butter.\n",
      ".. Das Wasser und Pfeffer , bis die Sauce geben und in einer in einem Schneebesen mischen. Die Zwiebel. Das Gemüse in der Zwischenzeit die Butter Pfanne. Das Gemüse\n",
      "\n",
      "100/100 [==============================] - 476s 5s/step - loss: 1.7303 - dense_41_loss: 1.7303\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 1.6950 - dense_41_loss: 1.6950generated text:\n",
      "\n",
      "250 g Reis [ANTISPACE] ∆ [ANTISPACE] s [ANTISPACE] ) , ( [ANTISPACE] . [UPPER] die Zucchini [ANTISPACE] ∆ [ANTISPACE] ∆ [ANTISPACE] , ( [ANTISPACE] ∆ [ANTISPACE] ) [ANTISPACE] ( [ANTISPACE] 100 g Zuck §$er [ANTISPACE] ∆ [ANTISPACE] ∆ [ANTISPACE] 1 Pris §$e [ANTISPACE] n [ANTISPACE] ∆ [ANTISPACE] ∆ [ANTISPACE] 1 [WHITESPACE] Banan §$e [ANTISPACE] ∆ [INSTRUCTIONS] [UPPER] das Ganz §$e Würfel schneid §$en [ANTISPACE] . [ANTISPACE] . [ANTISPACE] , entkern §$en [ANTISPACE] , mit dem restlich §$e Schüssel mit dem Teig verknet §$en lass §$en [ANTISPACE] . Salz und die Sahne und den [UKN] mit den Knoblauch und in ein §$em Topf geb §$en in ein §$e Schüssel geb §$en [ANTISPACE] ) [ANTISPACE] ª [ANTISPACE] - [ANTISPACE] . [UPPER] die Banan §$en und Pfeffer und in die Kartoffeln [ANTISPACE] , mit Wasser [ANTISPACE] . [ANTISPACE] . \n",
      "\n",
      "The other detokenizer ...\n",
      "250 g Reis\n",
      "\n",
      "s) , (. Die Zucchini\n",
      "\n",
      "\n",
      "\n",
      ", (\n",
      "\n",
      ")(100 g Zucker\n",
      "\n",
      "\n",
      "\n",
      "1 Prisen\n",
      "\n",
      "\n",
      "\n",
      "1  Banane\n",
      "\n",
      " [INSTRUCTIONS] Das Ganze Würfel schneiden.., entkernen, mit dem restliche Schüssel mit dem Teig verkneten lassen. Salz und die Sahne und den [UKN] mit den Knoblauch und in einem Topf geben in eine Schüssel geben)\n",
      "-. Die Bananen und Pfeffer und in die Kartoffeln, mit Wasser.. \n",
      "\n",
      "100/100 [==============================] - 494s 5s/step - loss: 1.6950 - dense_41_loss: 1.6950\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 1.6400 - dense_41_loss: 1.6400generated text:\n",
      "\n",
      "250 g Reis [ANTISPACE] ∆ [ANTISPACE] 1 TL Senf [ANTISPACE] ∆ [ANTISPACE] 1 groß §$e [ANTISPACE] ∆ [ANTISPACE] ∆ [ANTISPACE] 1 [WHITESPACE] Ei [ANTISPACE] ( [ANTISPACE] n [ANTISPACE] , ( [ANTISPACE] 100 g Zuck §$er [ANTISPACE] ∆ [ANTISPACE] ∆ [ANTISPACE] 1 EL Butt §$er [ANTISPACE] ∆ [ANTISPACE] 1 TL Zuck §$er [ANTISPACE] ∆ [INSTRUCTIONS] [UPPER] in klein schneid §$en [ANTISPACE] . Ω [ANTISPACE] . [ANTISPACE] . [UPPER] den Ofen geb §$en [ANTISPACE] [UPPER] das Wasser mit Zuck §$er und Pfeffer [ANTISPACE] , dann in ein §$em Topf zum [UPPER] die Mass §$e in Würfel schneid §$en [ANTISPACE] Ω [ANTISPACE] . [UPPER] die Eier verquirl §$en schneid §$en [ANTISPACE] . [ANTISPACE] . [ANTISPACE] . [ANTISPACE] [UPPER] in den Teig verarbeit §$en [ANTISPACE] , Pfeffer abschmeck §$en lass §$en [ANTISPACE] . [UPPER] das Olivenöl [ANTISPACE] , trock §$en und in die Hälft §$e Auflaufform schichten [ANTISPACE] . ∆ [ANTISPACE] . [UPPER] den Teig zu den Saft mit ein §$em Topf mit Backpapier ausgelegt §$e in der Form lös §$en und die Form leg §$en und in die Mass §$e Scheib §$en und mit der Mühle [ANTISPACE] ª [ANTISPACE] ª [ANTISPACE] . [UPPER] den Backofen ca [ANTISPACE] . [ANTISPACE] C back §$en [ANTISPACE] [UPPER] den Boden mit Salz und auf ein §$em Topf vom Strunk [ANTISPACE] . [ANTISPACE] . \n",
      "\n",
      "The other detokenizer ...\n",
      "250 g Reis\n",
      "\n",
      "1 TL Senf\n",
      "\n",
      "1 große\n",
      "\n",
      "\n",
      "\n",
      "1  Ei(n, (100 g Zucker\n",
      "\n",
      "\n",
      "\n",
      "1 EL Butter\n",
      "\n",
      "1 TL Zucker\n",
      "\n",
      " [INSTRUCTIONS] In klein schneiden. \n",
      ".. Den Ofen gebenDas Wasser mit Zucker und Pfeffer, dann in einem Topf zum Die Masse in Würfel schneiden\n",
      ". Die Eier verquirlen schneiden...In den Teig verarbeiten, Pfeffer abschmecken lassen. Das Olivenöl, trocken und in die Hälfte Auflaufform schichten. \n",
      "\n",
      ". Den Teig zu den Saft mit einem Topf mit Backpapier ausgelegte in der Form lösen und die Form legen und in die Masse Scheiben und mit der Mühle\n",
      "\n",
      ". Den Backofen ca.C backenDen Boden mit Salz und auf einem Topf vom Strunk.. \n",
      "\n",
      "100/100 [==============================] - 486s 5s/step - loss: 1.6400 - dense_41_loss: 1.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 1.5799 - dense_41_loss: 1.5799generated text:\n",
      "\n",
      "250 g Reis [ANTISPACE] / [ANTISPACE] s [ANTISPACE] ∆ [ANTISPACE] ∆ [ANTISPACE] ∆ [ANTISPACE] 2 Pck [ANTISPACE] 250 g Butt §$er , weiß [ANTISPACE] . Sahnesteif [ANTISPACE] ∆ [ANTISPACE] ∆ [ANTISPACE] ∆ [ANTISPACE] 4 [WHITESPACE] Zitron §$e [ANTISPACE] ( [ANTISPACE] ) [ANTISPACE] 1 Pck [ANTISPACE] 1 Pris §$e [ANTISPACE] ∆ [ANTISPACE] , gehack §$t [ANTISPACE] ∆ [ANTISPACE] 2 [WHITESPACE] Knoblauchzeh §$e , ( [ANTISPACE] ) [ANTISPACE] ∆ [ANTISPACE] . B [ANTISPACE] ∆ [ANTISPACE] 150 g Butt §$er , gehäuft §$e [ANTISPACE] n [ANTISPACE] ∆ [ANTISPACE] etwas Öl ( [ANTISPACE] ) , ( [ANTISPACE] ∆ [INSTRUCTIONS] [UPPER] die Zwiebel schäl §$en [ANTISPACE] , in Salzwasser koch §$en und in ein §$em Sieb abtropf §$en [ANTISPACE] , Knoblauch und mit Zuck §$er Schüssel geb §$en und Pfeffer würz §$en [ANTISPACE] . [UPPER] mit etwas Öl anbraten [ANTISPACE] ª [ANTISPACE] . ª [ANTISPACE] . [UPPER] dann die Kartoffeln in ein §$er Pfann §$e Auflaufform geb §$en und in ein §$em Sieb streich §$en und den Backofen auf der Pfann §$e Auflaufform geb §$en [ANTISPACE] [UPPER] die Zwiebeln geb §$en und Pfeffer und mit dem restlich §$en [ANTISPACE] , mit dem [UPPER] die Form auf dem [UPPER] mit den Rest des [UKN] auf die Hälft §$e [ANTISPACE] . \n",
      "\n",
      "The other detokenizer ...\n",
      "250 g Reis/s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2 Pck250 g Butter , weiß. Sahnesteif\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4  Zitrone()1 Pck1 Prise\n",
      "\n",
      ", gehackt\n",
      "\n",
      "2  Knoblauchzehe , ()\n",
      "\n",
      ". B\n",
      "\n",
      "150 g Butter , gehäuften\n",
      "\n",
      "etwas Öl () , (\n",
      "\n",
      " [INSTRUCTIONS] Die Zwiebel schälen, in Salzwasser kochen und in einem Sieb abtropfen, Knoblauch und mit Zucker Schüssel geben und Pfeffer würzen. Mit etwas Öl anbraten\n",
      ". \n",
      ". Dann die Kartoffeln in einer Pfanne Auflaufform geben und in einem Sieb streichen und den Backofen auf der Pfanne Auflaufform gebenDie Zwiebeln geben und Pfeffer und mit dem restlichen, mit dem Die Form auf dem Mit den Rest des [UKN] auf die Hälfte. \n",
      "\n",
      "100/100 [==============================] - 486s 5s/step - loss: 1.5799 - dense_41_loss: 1.5799\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 1.5265 - dense_41_loss: 1.5265generated text:\n",
      "\n",
      "250 g Reis [ANTISPACE] ∆ [ANTISPACE] 1 [WHITESPACE] Ei [ANTISPACE] ( [ANTISPACE] er [ANTISPACE] ) [ANTISPACE] ∆ [ANTISPACE] 1 klein §$e Rot §$e , weiß §$e [ANTISPACE] ∆ [ANTISPACE] 1 Becher Crème fraîche [ANTISPACE] ∆ [ANTISPACE] 250 ml Gemüsebrühe [ANTISPACE] ∆ [ANTISPACE] 100 g Zuck §$er [ANTISPACE] ∆ [ANTISPACE] 100 g Zuck §$er [ANTISPACE] ∆ [ANTISPACE] 100 g Zuck §$er [ANTISPACE] ∆ [ANTISPACE] 200 g Zuck §$er [ANTISPACE] ∆ Salz [ANTISPACE] ∆ [ANTISPACE] 2 TL Backpulver [ANTISPACE] ∆ [ANTISPACE] 100 g Schlagsahne [ANTISPACE] ∆ [INSTRUCTIONS] [UPPER] den Zuck §$er die Banan §$en und Backpulver [ANTISPACE] . Ω [ANTISPACE] , Zuck §$er mit Backpulver vermisch §$en [ANTISPACE] . [ANTISPACE] , mit den Teig verknet §$en [ANTISPACE] . Vanillezucker schaumig aufschlagen [ANTISPACE] ∆ [ANTISPACE] [UPPER] ein §$e Springform ( [ANTISPACE] ª [ANTISPACE] ° [ANTISPACE] C Ober [ANTISPACE] - und den Teig entsteh §$t [ANTISPACE] [UPPER] die [UKN] [ANTISPACE] . [ANTISPACE] . [UPPER] mit Backpapier geb §$en [ANTISPACE] , die Eier trenn §$en [ANTISPACE] . [ANTISPACE] . [UPPER] die Gelatine in ein §$e Auflaufform verteil §$en [ANTISPACE] . \n",
      "\n",
      "The other detokenizer ...\n",
      "250 g Reis\n",
      "\n",
      "1  Ei(er)\n",
      "\n",
      "1 kleine Rote , weiße\n",
      "\n",
      "1 Becher Crème fraîche\n",
      "\n",
      "250 ml Gemüsebrühe\n",
      "\n",
      "100 g Zucker\n",
      "\n",
      "100 g Zucker\n",
      "\n",
      "100 g Zucker\n",
      "\n",
      "200 g Zucker\n",
      "\n",
      " Salz\n",
      "\n",
      "2 TL Backpulver\n",
      "\n",
      "100 g Schlagsahne\n",
      "\n",
      " [INSTRUCTIONS] Den Zucker die Bananen und Backpulver. \n",
      ", Zucker mit Backpulver vermischen., mit den Teig verkneten. Vanillezucker schaumig aufschlagen\n",
      "\n",
      "Eine Springform (\n",
      "°C Ober- und den Teig entstehtDie [UKN].. Mit Backpapier geben, die Eier trennen.. Die Gelatine in eine Auflaufform verteilen. \n",
      "\n",
      "100/100 [==============================] - 492s 5s/step - loss: 1.5265 - dense_41_loss: 1.5265\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 1.4170 - dense_41_loss: 1.4170generated text:\n",
      "\n",
      "250 g Reis [ANTISPACE] ∆ [ANTISPACE] 1 EL Butt §$er [ANTISPACE] ∆ [ANTISPACE] 1 [WHITESPACE] Paprikaschot §$e [ANTISPACE] ( [ANTISPACE] n [ANTISPACE] ) [ANTISPACE] ∆ [ANTISPACE] 500 ml Sahne [ANTISPACE] ∆ [ANTISPACE] 1 TL Backpulver [ANTISPACE] ∆ [ANTISPACE] 1 TL Paprikapulver [ANTISPACE] ∆ [ANTISPACE] 1 EL Öl [ANTISPACE] , frisch §$er [ANTISPACE] ∆ [ANTISPACE] 3 Stiel §$e [ANTISPACE] / [ANTISPACE] n [ANTISPACE] ∆ [ANTISPACE] 2 EL Zuck §$er [ANTISPACE] ∆ Butt §$er [ANTISPACE] 1 Pris §$e [ANTISPACE] / [ANTISPACE] n [ANTISPACE] / [ANTISPACE] n [ANTISPACE] n [ANTISPACE] ∆ Fet §$t [ANTISPACE] ∆ [INSTRUCTIONS] [UPPER] das Mehl und den Zuck §$er [ANTISPACE] , Zuck §$er schaumig aufschlagen [ANTISPACE] ∆ [ANTISPACE] [UPPER] die Form für den Zuck §$er schmelz §$en und Zuck §$er schaumig schlag §$en [ANTISPACE] . [UPPER] den Teig in den Kuchen mit dem Boden mit der Butt §$er und den Zuck §$er die Birn §$en schneid §$en und in ein §$er groß §$en Pfann §$e in [UPPER] in klein §$en [ANTISPACE] . [UPPER] rühr §$en [ANTISPACE] . [UPPER] die Mass §$e und Zuck §$er in die Birn §$en auf die Butt §$er mit etwas abkühl §$en bring §$en [ANTISPACE] , den Teig rühr §$en und die Eier verquirl §$en [ANTISPACE] - [ANTISPACE] ° [ANTISPACE] C vorheizen [ANTISPACE] . [UPPER] nach dem Teig in ein §$er Schüssel geb §$en und die Füllung darauf geb §$en [ANTISPACE] , bis auf den [UKN] [ANTISPACE] . [UPPER] mit ein §$en Spritzbeutel füll §$en [ANTISPACE] . ∆ [ANTISPACE] [UPPER] für die Hälft §$e auf 200 ° [ANTISPACE] C\n",
      "\n",
      "The other detokenizer ...\n",
      "250 g Reis\n",
      "\n",
      "1 EL Butter\n",
      "\n",
      "1  Paprikaschote(n)\n",
      "\n",
      "500 ml Sahne\n",
      "\n",
      "1 TL Backpulver\n",
      "\n",
      "1 TL Paprikapulver\n",
      "\n",
      "1 EL Öl, frischer\n",
      "\n",
      "3 Stiele/n\n",
      "\n",
      "2 EL Zucker\n",
      "\n",
      " Butter1 Prise/n/nn\n",
      "\n",
      " Fett\n",
      "\n",
      " [INSTRUCTIONS] Das Mehl und den Zucker, Zucker schaumig aufschlagen\n",
      "\n",
      "Die Form für den Zucker schmelzen und Zucker schaumig schlagen. Den Teig in den Kuchen mit dem Boden mit der Butter und den Zucker die Birnen schneiden und in einer großen Pfanne in In kleinen. Rühren. Die Masse und Zucker in die Birnen auf die Butter mit etwas abkühlen bringen, den Teig rühren und die Eier verquirlen-°C vorheizen. Nach dem Teig in einer Schüssel geben und die Füllung darauf geben, bis auf den [UKN]. Mit einen Spritzbeutel füllen. \n",
      "\n",
      "Für die Hälfte auf 200 °C\n",
      "\n",
      "100/100 [==============================] - 510s 5s/step - loss: 1.4170 - dense_41_loss: 1.4170\n",
      "Epoch 8/100\n",
      " 64/100 [==================>...........] - ETA: 3:24 - loss: 1.3368 - dense_41_loss: 1.3368"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    tokey.generator(32,512), \n",
    "    verbose=1, \n",
    "    steps_per_epoch = 100,\n",
    "    callbacks = [text_gen_callback],\n",
    "    epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-fancy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dab7fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recipe",
   "language": "python",
   "name": "recipe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
